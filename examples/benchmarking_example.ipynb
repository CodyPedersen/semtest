{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking (running in direct, non-framework mode)\n",
    "\n",
    "Demonstrates the ability to benchmark llm responses against an given semantic expectation directly. Later functionality will allow you to run in framework mode, in which benchmarks are auto-executed using defined fixtures as parameters.\n",
    "\n",
    "#### Requires:\n",
    "- .env file in project directory configured with `OPENAI_API_KEY`, `BASE_URL`, `DEFAULT_EMBEDDING_MODEL` (or using defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import semtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined semantic expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation = \"A dog is in the background of the photograph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mock_llm_response():\n",
    "    responses = [\n",
    "        \"There's a dog in the background of the photo\",\n",
    "        \"In the background of the photo is a dog\",\n",
    "        \"There's an animal in the background of the photo and it's a dog.\"\n",
    "    ]\n",
    "    for response in responses:\n",
    "        yield response\n",
    "\n",
    "mock_llm_response_generator = mock_llm_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorate the function to act as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@semtest.benchmark(\n",
    "    semantic_expectation=expectation,\n",
    "    iterations=3\n",
    ")\n",
    "def mock_prompt_benchmark():\n",
    "    # Here's where the llm call with the designated prompt would occur\n",
    "    mocked_llm_response = next(mock_llm_response_generator)\n",
    "    print(f\"llm responded with `{mocked_llm_response}`\")\n",
    "    return mocked_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res: semtest.Benchmark = mock_prompt_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.benchmarks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.benchmarks_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
