{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking (running in direct, non-framework mode)\n",
    "\n",
    "Demonstrates the ability to benchmark llm responses against an given semantic expectation directly. Later functionality will allow you to run in framework mode, in which benchmarks are auto-executed using defined fixtures as parameters.\n",
    "\n",
    "#### Requires:\n",
    "- .env file in project directory configured with `OPENAI_API_KEY`, `BASE_URL`, `DEFAULT_EMBEDDING_MODEL` (or using defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "import semtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined semantic expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation = \"A dog is in the background of the photograph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm_response_prompt1():\n",
    "    responses = [\n",
    "        \"There's a dog in the background of the photo\",\n",
    "        \"In the background of the photo is a dog\",\n",
    "        \"There's an animal in the background of the photo and it's a dog.\"\n",
    "    ]\n",
    "    for response in responses:\n",
    "        yield response\n",
    "\n",
    "mock_llm_response_generator_prompt_1 = mock_llm_response_prompt1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llm_response_prompt2():\n",
    "    responses = [\n",
    "        \"In the background of the photograph there is a furry animal\",\n",
    "        \"In the foreground there is a human, and I see a dog in the background of the photograph\",\n",
    "        \"There are two dogs in the background of the image\"\n",
    "    ]\n",
    "    for response in responses:\n",
    "        yield response\n",
    "\n",
    "mock_llm_response_generator_prompt_2 = mock_llm_response_prompt2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorate the function to act as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@semtest.benchmark(\n",
    "    semantic_expectation=expectation,\n",
    "    iterations=3\n",
    ")\n",
    "def mock_prompt_benchmark_prompt_1():\n",
    "    \"\"\"A better prompt/temperature/config\"\"\"\n",
    "\n",
    "    # intermediary logic ...\n",
    "    \n",
    "    mocked_llm_response = next(mock_llm_response_generator_prompt_1)  # mock llm response\n",
    "\n",
    "    # user validations ...\n",
    "\n",
    "    return mocked_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@semtest.benchmark(\n",
    "    semantic_expectation=expectation,\n",
    "    iterations=3\n",
    ")\n",
    "def mock_prompt_benchmark_prompt_2():\n",
    "    \"\"\"A slightly worse prompt/temperature/config\"\"\"\n",
    "\n",
    "    # intermediary logic ...\n",
    "    \n",
    "    mocked_llm_response = next(mock_llm_response_generator_prompt_2)  # mock llm response\n",
    "\n",
    "    # user validations ...\n",
    "\n",
    "    return mocked_llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_result: semtest.BenchmarkRunner = mock_prompt_benchmark_prompt_1()\n",
    "prompt_2_result: semtest.BenchmarkRunner = mock_prompt_benchmark_prompt_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func='mock_prompt_benchmark_prompt_1' iterations=3 comparator='cosine_similarity' expectation_input='A dog is in the background of the photograph' benchmarks=SemanticMetrics(responses=[\"There's a dog in the background of the photo\", 'In the background of the photo is a dog', \"There's an animal in the background of the photo and it's a dog.\"], semantic_distances=[np.float64(0.8688688326426504), np.float64(0.8313293293059392), np.float64(0.7697516222847918)], mean_semantic_distance=np.float64(0.8233165947444604), median_semantic_distance=np.float64(0.8313293293059392))\n",
      "func='mock_prompt_benchmark_prompt_2' iterations=3 comparator='cosine_similarity' expectation_input='A dog is in the background of the photograph' benchmarks=SemanticMetrics(responses=['In the background of the photograph there is a furry animal', 'In the foreground there is a human, and I see a dog in the background of the photograph', 'There are two dogs in the background of the image'], semantic_distances=[np.float64(0.7213289868782804), np.float64(0.7030024510268936), np.float64(0.7529891407174136)], mean_semantic_distance=np.float64(0.7257735262075292), median_semantic_distance=np.float64(0.7213289868782804))\n"
     ]
    }
   ],
   "source": [
    "print(prompt_1_result.metrics)\n",
    "print(prompt_2_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"func\": \"mock_prompt_benchmark_prompt_1\",\n",
      "  \"iterations\": 3,\n",
      "  \"comparator\": \"cosine_similarity\",\n",
      "  \"expectation_input\": \"A dog is in the background of the photograph\",\n",
      "  \"benchmarks\": {\n",
      "    \"responses\": [\n",
      "      \"There's a dog in the background of the photo\",\n",
      "      \"In the background of the photo is a dog\",\n",
      "      \"There's an animal in the background of the photo and it's a dog.\"\n",
      "    ],\n",
      "    \"semantic_distances\": [\n",
      "      0.8688688326426504,\n",
      "      0.8313293293059392,\n",
      "      0.7697516222847918\n",
      "    ],\n",
      "    \"mean_semantic_distance\": 0.8233165947444604,\n",
      "    \"median_semantic_distance\": 0.8313293293059392\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"func\": \"mock_prompt_benchmark_prompt_2\",\n",
      "  \"iterations\": 3,\n",
      "  \"comparator\": \"cosine_similarity\",\n",
      "  \"expectation_input\": \"A dog is in the background of the photograph\",\n",
      "  \"benchmarks\": {\n",
      "    \"responses\": [\n",
      "      \"In the background of the photograph there is a furry animal\",\n",
      "      \"In the foreground there is a human, and I see a dog in the background of the photograph\",\n",
      "      \"There are two dogs in the background of the image\"\n",
      "    ],\n",
      "    \"semantic_distances\": [\n",
      "      0.7213289868782804,\n",
      "      0.7030024510268936,\n",
      "      0.7529891407174136\n",
      "    ],\n",
      "    \"mean_semantic_distance\": 0.7257735262075292,\n",
      "    \"median_semantic_distance\": 0.7213289868782804\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(prompt_1_result.metrics_json)\n",
    "print(prompt_2_result.metrics_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
